<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Wan2.1</title>
      <link href="/2025/07/01/Wan2-1/"/>
      <url>/2025/07/01/Wan2-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Wan2-1"><a href="#Wan2-1" class="headerlink" title="Wan2.1"></a>Wan2.1</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Adam</title>
      <link href="/2025/04/23/Adam/"/>
      <url>/2025/04/23/Adam/</url>
      
        <content type="html"><![CDATA[<h3 id="Adam-A-Method-for-Stochastic-Optimization"><a href="#Adam-A-Method-for-Stochastic-Optimization" class="headerlink" title="Adam: A Method for Stochastic Optimization"></a>Adam: A Method for Stochastic Optimization</h3><p>Adam（Adaptive Moment Estimation，自适应矩估计）是一种广泛应用于深度学习中的一阶优化算法，结合了动量法和RMSProp的优点，通过自适应地调整学习率来加速梯度下降的收敛。</p><h3 id="不同梯度下降算法的局限性："><a href="#不同梯度下降算法的局限性：" class="headerlink" title="不同梯度下降算法的局限性："></a>不同梯度下降算法的局限性：</h3><ul><li>传统梯度下降: 使用固定的学习率，可能导致收敛过慢或陷入局部最小值。</li><li>AdaGrad: 通过累计梯度平方来缩放学习率，适合处理稀疏数据，但学习率可能过早衰减，导致收敛停滞。</li><li>RMSProp: 引入了指数平均来解决AdaGrad学习率衰减过快的问题，但仍缺乏对梯度方向的动态调整。</li></ul><h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p>Adam的核心思想是通过维护两个移动平均两来优化参数:</p><ul><li>一阶矩(动量)：即梯度的指数移动平均，用于捕捉梯度的方向。</li><li>二阶矩(未中心化的方差)：即梯度平方的指数移动平均，用于自适应地调整学习率</li></ul><p>通过这两者的结合，Adam能够：</p><ul><li>根据梯度的大小动态调整学习率（类似RMSProp）。</li><li>利用动量加速梯度下降（类似动量法）。</li><li>提供偏差校正机制，确保在训练初期（当移动平均还未充分“预热”时）也能表现良好。</li></ul><h3 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h3><p>输入参数：</p><ul><li>$\alpha$ : 学习率</li><li>$\beta_1$ : 一阶矩衰减率， 0.9</li><li>$\beta_2$ :  二阶矩衰减率， 0.999</li><li>ϵ : 数值稳定性常数，防止除以0</li></ul><h3 id="迭代更新"><a href="#迭代更新" class="headerlink" title="迭代更新"></a>迭代更新</h3><p>初始化一阶矩$m_0$,二阶矩$v_0$均为0，时间步$t$为0</p><ol><li><p>计算梯度：<br>$g_t &#x3D; \nabla f(\theta_{t-1})$</p></li><li><p>更新一阶矩（动量）<br>$m_t &#x3D; \beta_1 m_{t-1} + (1-\beta_1)g_t$<br>（这是梯度的指数移动平均，类似动量法）</p></li><li><p>更新二阶矩（方差）<br>$v_t &#x3D; \beta_2 v_{t-1} + (1-\beta_2)g_t^2$<br>（这是梯度平方的指数移动平均，类似RMSProp） </p></li><li><p>偏差校正 （修正初期偏差）</p></li></ol><p>$\hat{m_t} &#x3D; \frac{m_t}{1 - \beta_1^t}$<br>$\hat{v_t} &#x3D; \frac{v_t}{1 - \beta_2^t}$</p><p>（早期动量都是0，需要矫正一下）</p><ol start="5"><li>更新参数</li></ol><p>$\theta_t &#x3D; \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">初始化：θ<span class="number">0</span>, m0 = <span class="number">0</span>, v0 = <span class="number">0</span>, t = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> 未收敛 do:  </span><br><span class="line">    t ← t + <span class="number">1</span>  </span><br><span class="line">    gt ← ∇f(θt-<span class="number">1</span>)  <span class="comment"># 计算梯度</span></span><br><span class="line">    mt ← β<span class="number">1</span> * mt-<span class="number">1</span> + (<span class="number">1</span> - β<span class="number">1</span>) * gt  <span class="comment"># 更新一阶矩</span></span><br><span class="line">    vt ← β<span class="number">2</span> * vt-<span class="number">1</span> + (<span class="number">1</span> - β<span class="number">2</span>) * gt²  <span class="comment"># 更新二阶矩</span></span><br><span class="line">    m̂t ← mt / (<span class="number">1</span> - β<span class="number">1</span>^t)  <span class="comment"># 偏差校正</span></span><br><span class="line">    v̂t ← vt / (<span class="number">1</span> - β<span class="number">2</span>^t)  <span class="comment"># 偏差校正</span></span><br><span class="line">    θt ← θt-<span class="number">1</span> - α * m̂t / (√v̂t + ε)  <span class="comment"># 更新参数</span></span><br><span class="line">end <span class="keyword">while</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
